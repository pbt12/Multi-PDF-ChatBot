{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(groq_api_key =  os.getenv(\"GROQ_API_KEY\"),model=\"mixtral-8x7b-32768\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an assistant. You hate to talk give lot of words, but gives precise, concise and very short but answers all of the questions'), HumanMessage(content='Hi, How are you ?')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are an assistant. You hate to talk give lot of words, but gives precise, concise and very short but answers all of the questions\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', human)\n",
    "    ]\n",
    ")\n",
    "prompt.invoke(input = {\"text\":\"Hi, How are you ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'll do my best to provide concise and accurate answers to your questions. Let's get started!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | chat\n",
    "response = chain.invoke(input = {\"text\":\"Hi\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something you would like to ask or talk about? I'm here to help with any questions you might have about writing, grammar, or language in general. Just let me know how I can assist you.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, api_key = os.getenv(\"GROQ_API_KEY\"), model_name = \"mixtral-8x7b-32768\")\n",
    "print(chat.invoke(\"Hi\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='What is the capital Of India ?')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"What is the capital Of India ?\")\n",
    "    ]\n",
    ")\n",
    "template.invoke(input = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi. Established as the capital in 1911, New Delhi is the seat of government and the center of politics in India. It is located within the National Capital Territory of Delhi and is one of the 11 districts of the NCT. New Delhi is known for its rich history, diverse culture, and vibrant atmosphere. It is home to numerous landmarks, including the India Gate, Red Fort, and the Lotus Temple.\n"
     ]
    }
   ],
   "source": [
    "chain = template | chat\n",
    "response = chain.invoke(input = {})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template\n",
    "system = \"You are helpful AI assistant. You dont like to respond much, every text you give costs a lot. \\\n",
    "    So, you try to give response as small, precise, concise as possible, without missing out important response. \\\n",
    "    If you understand this role, follow from first response itself. Respond with as little text as possible \\\n",
    "    (sometimes simple ok, yes, no, understood...(not only these words, but the words or sentences that might best apt for the question)\\\n",
    "        like these words alos suffices, but if required you have to give reason as well).\"\n",
    "\n",
    "system1 = \"You are helpful assistant\"\n",
    "\n",
    "human = f'Hi'\n",
    "\n",
    "startup_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',system),\n",
    "        ('human', human)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_model = ChatGroq(model='llama-3.1-405b-reasoning', api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chain\n",
    "chain = startup_template | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(input={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
       " 'kwargs': {'content': 'Understood. Hello!',\n",
       "  'response_metadata': {'token_usage': {'completion_tokens': 6,\n",
       "    'prompt_tokens': 128,\n",
       "    'total_tokens': 134,\n",
       "    'completion_time': 0.009375,\n",
       "    'prompt_time': 0.007400614,\n",
       "    'queue_time': None,\n",
       "    'total_time': 0.016775614},\n",
       "   'model_name': 'mixtral-8x7b-32768',\n",
       "   'system_fingerprint': 'fp_c5f20b5bb1',\n",
       "   'finish_reason': 'stop',\n",
       "   'logprobs': None},\n",
       "  'type': 'ai',\n",
       "  'id': 'run-305b2acc-0be7-4d84-be9f-998e8e40feae-0',\n",
       "  'usage_metadata': {'input_tokens': 128,\n",
       "   'output_tokens': 6,\n",
       "   'total_tokens': 134},\n",
       "  'tool_calls': [],\n",
       "  'invalid_tool_calls': []}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "groq = ChatGroq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
